{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def sigmoid(z):\n","    s = 1 / (1 + np.exp(-z))\n","    return s\n","\n","def relu(z):\n","    s = np.max(0,z)\n","    return s"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def initialize_parameters(n_x, n_h, n_y):\n","\n","    np.random.seed(1)\n","\n","    W1 = np.random.randn(n_h, n_x)*0.01\n","    b1 = np.zeros((n_h, 1))\n","    W2 = np.random.randn(n_y, n_h)*0.01\n","    b2 = np.zeros((n_y, 1))\n","\n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters \n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def initialize_parameters_deep(layer_dims):\n","\n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims)\n","\n","    for l in range(1, L):\n","        \n","        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n","        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","        \n","    return parameters"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def linear_forward(A, W, b):\n","\n","    Z = np.dot(W,A) + b\n","    cache = (A, W, b)\n","\n","    return Z, cache"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \n","    if activation == \"sigmoid\":\n","       \n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","        \n","    \n","    elif activation == \"relu\":\n","        \n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","        \n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def L_model_forward(X, parameters):\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2                  \n","    \n","    for l in range(1, L):\n","        A_prev = A \n","        \n","        W = parameters['W' + str(l)]\n","        b = parameters['b' + str(l)]\n","        A, cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","        caches.append(cache)\n","       \n","    W = parameters['W' + str(L)]\n","    b = parameters['b' + str(L)]\n","    AL, cache = linear_activation_forward(A, W, b, activation = \"sigmoid\")\n","    caches.append(cache)\n","          \n","    return AL, caches"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def compute_cost(AL, Y):\n","\n","    m = Y.shape[1]\n","\n","    cost = - (1/m) * (np.dot(Y,np.log(AL).T) + np.dot((1 - Y), np.log(1 - AL).T))\n","\n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","\n","    return cost"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def linear_backward(dZ, cache):\n","\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    dW = (1/m) * (np.dot(dZ, A_prev.T))\n","    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(W.T, dZ)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def linear_activation_backward(dA, cache, activation):\n","\n","    linear_cache, activation_cache = cache\n","    \n","    if activation == \"relu\":\n","        \n","        dZ = relu_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        \n","        \n","    elif activation == \"sigmoid\":\n","        \n","        dZ = sigmoid_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        \n","    \n","    return dA_prev, dW, db"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def L_model_backward(AL, Y, caches):\n","\n","    grads = {}\n","    L = len(caches) \n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) \n","    \n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","    \n","    current_cache = caches[L-1]\n","    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n","    grads[\"dA\" + str(L-1)] =  dA_prev_temp\n","    grads[\"dW\" + str(L)] = dW_temp\n","    grads[\"db\" + str(L)] = db_temp\n","\n","    for l in reversed(range(L-1)):\n","\n","        current_cache = caches [l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","\n","    return grads"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def update_parameters(params, grads, learning_rate):\n","\n","    parameters = params.copy()\n","    L = len(parameters) // 2 \n","\n","    \n","    for l in range(L):\n","        \n","        parameters[\"W\" + str(l+1)] = params['W' + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l+1)] = params['b' + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n","        \n","    return parameters"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n","\n","    np.random.seed(1)\n","    grads = {}\n","    costs = []                              \n","    m = X.shape[1]                           \n","    (n_x, n_h, n_y) = layers_dims\n","\n","    parameters = initialize_parameters(n_x, n_h, n_y)\n","\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","    \n","\n","    for i in range(0, num_iterations):\n","\n","        A1, cache1 = linear_activation_forward(X, W1, b1, activation = 'relu')\n","        A2, cache2 = linear_activation_forward(A1, W2, b2, activation = 'sigmoid')\n","        \n","        cost = compute_cost(A2, Y)\n","\n","        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n","\n","        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = 'sigmoid')\n","        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = 'relu')\n","        \n","        grads['dW1'] = dW1\n","        grads['db1'] = db1\n","        grads['dW2'] = dW2\n","        grads['db2'] = db2\n","        \n","        parameters = update_parameters(parameters, grads, learning_rate)\n","    \n","        W1 = parameters[\"W1\"]\n","        b1 = parameters[\"b1\"]\n","        W2 = parameters[\"W2\"]\n","        b2 = parameters[\"b2\"]\n","        \n","        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n","            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n","        if i % 100 == 0 or i == num_iterations:\n","            costs.append(cost)\n","\n","    return parameters, costs\n","       "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# parameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\n","# predictions_train = predict(train_x, train_y, parameters)\n","# predictions_test = predict(test_x, test_y, parameters)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n","\n","    np.random.seed(1)\n","    costs = []                         \n","\n","    parameters = initialize_parameters_deep(layers_dims)\n","    \n","    for i in range(0, num_iterations):\n","        \n","        AL, caches = L_model_forward(X, parameters)\n","        \n","        cost = compute_cost(AL, Y)\n","        \n","        grads = L_model_backward(AL, Y, caches)\n","        \n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","        \n","        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n","            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n","        if i % 100 == 0 or i == num_iterations:\n","            costs.append(cost)\n","    \n","    return parameters, costs"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n","# pred_train = predict(train_x, train_y, parameters)\n","# pred_test = predict(test_x, test_y, parameters)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d3ee1eecf4f60a39431daf7cc73333bba67933358b7f9d243b4a5c4aa9fb0709"}}},"nbformat":4,"nbformat_minor":2}
